{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import string\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "\n",
    "# #Read the sentences in seperate text files\n",
    "# with open ('short_nep//nep_337_21_nov.txt', 'r', encoding=\"utf-8\") as f:\n",
    "#     short_nepali_txt = f.read()\n",
    "    \n",
    "# with open ('short_nep//short_eng_nep_nov_21.txt', 'r', encoding=\"utf-8\") as f:\n",
    "#     short_eng_txt = f.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No unknown tokens are used here\n",
    "#specify language -- start and end tokens are appended accordingly\n",
    "#Vocab indices start from 1, 0 is saved for padding missing words in short sentences\n",
    "#ALL THE ADDITION OR DELITIONS OF SENTENCES SHOULD BE SYMMETRIC --> OCCURING TOGETHER IN BOTH CORPUS\n",
    "\n",
    "def pre_process(raw_text, lang):\n",
    "    \n",
    "    \n",
    "    no_quotes = ''.join(ch for ch in raw_text if ch not in set(string.punctuation))\n",
    "    \n",
    "    no_quotes = no_quotes[1:]  #replaces (\"\\ufeff\") --> a single character\n",
    "    \n",
    "    if lang == 'nep':\n",
    "        no_quotes = no_quotes.replace(\"|'\", '')\n",
    "        \n",
    "    else:\n",
    "        no_quotes = no_quotes.lower()   #all english words in lower case\n",
    "    \n",
    "    split_sents = no_quotes.split('\\n') #makes one single list of sentences\n",
    "    \n",
    "    \n",
    "    sent_list = [[i] for i in split_sents] #Make a seperate sublist within a list\n",
    "    \n",
    "    words_in_sent = [sent_list[i][0].split() for i in range(len(sent_list))] #Split each sentence into words\n",
    "    words_in_sent_seos = copy.deepcopy(words_in_sent) #Make a copy without interferring the above copy \n",
    "    \n",
    "    all_words = no_quotes.split() #each word in corpus is split seperately\n",
    "    most_common = [item for item in Counter(all_words).most_common()]\n",
    "    \n",
    "    if lang == 'nep':\n",
    "        words_in_sent_seos = [['एसओएस']+i for i in words_in_sent_seos] #append 'beginning of sentence token'\n",
    "        [i.append('ईओएस') for i in words_in_sent_seos] #Append 'end of sentence token'\n",
    "        all_words.extend(['एसओएस', 'ईओएस'])\n",
    "    \n",
    "    else:\n",
    "        words_in_sent_seos = [['start_o_s']+i for i in words_in_sent_seos] #append 'beginning of sentence token'\n",
    "        [i.append('end_o_s') for i in words_in_sent_seos] #Append 'end of sentence token'\n",
    "        all_words.extend(['start_o_s', 'end_o_s'])\n",
    "        \n",
    "    vocab = set(all_words) # set containing all the words\n",
    "    \n",
    "    a_list = [i+1 for i in range(len(vocab)+1)] #list range, starts from 1, 0 index is saved for padding purpose \n",
    "    idx_2_word = dict(zip(a_list, list(vocab))) #look up dict assigning an index to each unique word\n",
    "    word_2_idx = {v:k for k, v in idx_2_word.items()} #reversed mapping\n",
    "    \n",
    "    \n",
    "    maxList = (max(words_in_sent_seos,key=len))\n",
    "    maxLength = len(maxList)\n",
    "    \n",
    "    return (words_in_sent_seos, vocab, idx_2_word, word_2_idx, most_common, maxList, maxLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU switch\n",
    "import torch\n",
    "\n",
    "use_gpu = True\n",
    "\n",
    "\n",
    "if use_gpu:\n",
    "    dtype = torch.cuda.FloatTensor \n",
    "else:\n",
    "    dtype = torch.FloatTensor\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note we will use different sets of embedding for source and target\n",
    "#Embeddings are not trained\n",
    "#Here the very first embedding (0 index) is used for padding, 'pad' subsequently added to vocab\n",
    "#Hence vocab +1 embedding are needed\n",
    "def get_embeds(vocab, d_model):\n",
    "    embeds = nn.Embedding(len(vocab)+1, d_model)\n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentences --> seq of words --> indices --> embedding stack\n",
    "#Length of each sentence is later used to develop appropriate mask --> num of words/sentence are differ \n",
    "#paddings will be used to keep sequence length the same\n",
    "def sent_2_idxs(sentences, embedding, word_2_idx_dict):\n",
    "    \n",
    "    \n",
    "    batch_tensor = torch.FloatTensor([]) #Empty tensor to get cumulate all sentence --> all word embeding\n",
    "    sent_to_idx =[] #Get the corresponding integers for each word (word to index)\n",
    "    num_sentences = len(sentences) #Just cache number of actual words in each sentence, needed for padding\n",
    "    embedding_stack = []  #Tensors collected as lit --> Unnecessary\n",
    "    \n",
    "    \n",
    "    max_len = len(max(sentences, key=len))\n",
    "    \n",
    "    \n",
    "    all_zeros = [0 for i in range(max_len)] #predefine list of 0s --> simplifies padding\n",
    "    len_each_sent = [len(x) for x in sentences]  #Later will be needed for padding to nullify attention weights\n",
    "    \n",
    "    for j in range(num_sentences):\n",
    "        word_2_int = [word_2_idx_dict[i] for i in sentences[j]]   #Get indices\n",
    "        word_2_int = word_2_int + all_zeros[len(word_2_int):]  #Use 0 padding, when there are < max_len words \n",
    "        \n",
    "        sent_to_idx.append(word_2_int) #Collect all sentences-words as indices in a full list\n",
    "        \n",
    "        batch_tensor = torch.cat((batch_tensor, embedding(torch.LongTensor(word_2_int)).unsqueeze(0)), dim = 0) #Full stack tensor\n",
    "        \n",
    "        #This is redundant\n",
    "        #embedding_stack.append(embedding_e(torch.LongTensor(word_2_int)))\n",
    "    \n",
    "    \n",
    "    return sent_to_idx, len_each_sent, batch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get positional encodings\n",
    "\n",
    "def pos_enc(cur_batch, d_model):\n",
    "        \n",
    "    num_words = cur_batch.size(1)\n",
    "    \n",
    "    p_cache = []  #Catch all the positional encodes for position ij\n",
    "    \n",
    "    pos_embs = torch.zeros(num_words, d_model)\n",
    "    \n",
    "    \n",
    "    #pos--> position of the word\n",
    "    #i --> position within the word vector\n",
    "    for pos in range(num_words):\n",
    "        p_cache = []\n",
    "        for  i in range(d_model):\n",
    "            if i%2 == 0:\n",
    "\n",
    "                order_emb = np.sin(pos/(10000**(i/(d_model))))\n",
    "                p_cache.append(order_emb)\n",
    "                \n",
    "\n",
    "            else:\n",
    "                \n",
    "                order_emb = np.cos(pos/(10000**(i/(d_model))))\n",
    "                p_cache.append(order_emb)\n",
    "                \n",
    "        pos_embs[pos, :] = torch.tensor(p_cache)\n",
    "    return pos_embs\n",
    "    \n",
    "\n",
    "\n",
    "def add_pos_enc(cur_batch):\n",
    "\n",
    "    d_model = cur_batch.size(-1)\n",
    "    get_pos_enc = pos_enc(cur_batch, d_model)\n",
    "    input_add_pos = cur_batch + get_pos_enc\n",
    "    return input_add_pos\n",
    "\n",
    "\n",
    "#Alternative method to add positional encoding --> this doesn't require computing the encodings each time\n",
    "#when  model_dim and max of sequence legth is known\n",
    "d_model = 64\n",
    "some_dummy = torch.arange(2*20*d_model).view(-1, 20, d_model)\n",
    "ref_pos_enc = pos_enc(some_dummy, d_model).unsqueeze(0)\n",
    "\n",
    "\n",
    "def add_pos_enc_alt(cur_batch):\n",
    "\n",
    "    get_pos_enc = ref_pos_enc[0, :cur_batch.size(1), :]\n",
    "    input_add_pos = cur_batch + get_pos_enc\n",
    "    return input_add_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mask eliminates all the scores computations due to padding, where actual words were not used\n",
    "#tens --> tensor\n",
    "#Create pading mask when sentences in a batch have unqual length\n",
    "def get_mask_by_length(len_list):\n",
    "    tens_ones = torch.ones(len(len_list), max(len_list)) #all ones\n",
    "    tens_multipy = tens_ones * torch.tensor(len_list).view(tens_ones.size(0), -1) # multiply by length of each\n",
    "    tens_range = ([torch.arange(max(len_list)) for i in range(len(len_list))]) #returns list of tensors from 0 to max_len\n",
    "    tens_range = torch.stack(tens_range) #stack them to get final tensor\n",
    "    #print(tens_ones, tens_multipy, tens_range, tens_range)\n",
    "    return (tens_range < tens_multipy)*1 #finally get the mask\n",
    "\n",
    "\n",
    "#works only if paddings were all 0s\n",
    "def get_mask_by_batch(cur_batch):\n",
    "    #works only if all paddings are actually 0\n",
    "    return ((torch.sum(cur_batch, dim =-1))!=0)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_x =[1, 2, 5]\n",
    "get_mask_by_length(some_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def init_weights(d_model, use_gpu):\n",
    "    \n",
    "    if use_gpu:\n",
    "        Wq = Variable(torch.rand(d_model, d_model).cuda(), requires_grad=True)#, device=\"cuda\")\n",
    "        torch.nn.init.xavier_uniform_(Wq)\n",
    "        Wk = Variable(torch.rand(d_model, d_model).cuda(), requires_grad=True)#, device=\"cuda\")\n",
    "        torch.nn.init.xavier_uniform_(Wk)\n",
    "        Wv = Variable(torch.rand(d_model, d_model).cuda(), requires_grad=True)#, device=\"cuda\")\n",
    "        torch.nn.init.xavier_uniform_(Wv)\n",
    "        Wo = Variable(torch.rand(d_model, d_model).cuda(), requires_grad=True)#, device=\"cuda\")\n",
    "        torch.nn.init.xavier_uniform_(Wv)\n",
    "    else:\n",
    "        Wq = Variable(torch.rand(d_model, d_model), requires_grad=True)\n",
    "        torch.nn.init.xavier_uniform_(Wq)\n",
    "        Wk = Variable(torch.rand(d_model, d_model), requires_grad=True)\n",
    "        torch.nn.init.xavier_uniform_(Wk)\n",
    "        Wv = Variable(torch.rand(d_model, d_model), requires_grad=True)\n",
    "        torch.nn.init.xavier_uniform_(Wv)\n",
    "        Wo = Variable(torch.rand(d_model, d_model), requires_grad=True)\n",
    "        torch.nn.init.xavier_uniform_(Wv)\n",
    "    return Wq, Wk, Wv, Wo\n",
    "\n",
    "\n",
    "def init_FFN(d_model, int_vec_size, use_gpu):\n",
    "    \n",
    "    if use_gpu:\n",
    "        FF1 = Variable(torch.randn(d_model, int_vec_size).cuda(), requires_grad=True)#, device=\"cuda\")\n",
    "        torch.nn.init.xavier_uniform_(FF1)\n",
    "        FF2 = Variable(torch.randn(int_vec_size, d_model).cuda(), requires_grad=True)#, device=\"cuda\")\n",
    "        torch.nn.init.xavier_uniform_(FF2)\n",
    "        bFF1 = Variable(torch.zeros(1, int_vec_size).cuda(), requires_grad=True)#, device=\"cuda\")\n",
    "        bFF2 = Variable(torch.zeros(1, d_model).cuda(), requires_grad=True)#, device=\"cuda\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    else:\n",
    "        FF1 = Variable(torch.randn(d_model, int_vec_size), requires_grad=True)\n",
    "        torch.nn.init.xavier_uniform_(FF1)\n",
    "        FF2 = Variable(torch.randn(int_vec_size, d_model), requires_grad=True)\n",
    "        torch.nn.init.xavier_uniform_(FF2)\n",
    "        bFF1 = Variable(torch.zeros(1, int_vec_size), requires_grad=True)\n",
    "        bFF2 = Variable(torch.zeros(1, d_model), requires_grad=True)\n",
    "    \n",
    "    return FF1, FF2, bFF1, bFF2\n",
    "\n",
    "def init_fin_FFN(d_model, vocab, use_gpu):\n",
    "    fin_vec_size = len(list(vocab)) + 1\n",
    "    \n",
    "    if use_gpu:\n",
    "        wts = Variable(torch.randn(d_model, fin_vec_size).cuda(), requires_grad=True)\n",
    "        torch.nn.init.xavier_uniform_(wts)\n",
    "        biases = Variable(torch.zeros(1, fin_vec_size).cuda(), requires_grad=True)\n",
    "       \n",
    "    else:\n",
    "        wts = Variable(torch.randn(d_model, fin_vec_size), requires_grad=True)\n",
    "        torch.nn.init.xavier_uniform_(wts)\n",
    "        biases = Variable(torch.zeros(1, fin_vec_size), requires_grad=True)\n",
    "    \n",
    "    return wts, biases\n",
    "\n",
    "\n",
    "\n",
    "# def init_fin_FFN(d_model, vocab, use_gpu):\n",
    "    \n",
    "#     if use_gpu:\n",
    "#         int_vec_size = len(list(vocab)) + 1\n",
    "#         wts = Variable(torch.randn(d_model, int_vec_size).cuda(), requires_grad=True)\n",
    "#         torch.nn.init.xavier_uniform_(wts)\n",
    "#         biases = Variable(torch.zeros(1, int_vec_size).cuda(), requires_grad=True)\n",
    "    \n",
    "    \n",
    "#     else:\n",
    "#         int_vec_size = len(list(vocab)) + 1\n",
    "#         wts = Variable(torch.randn(d_model, int_vec_size), requires_grad=True)#*0.001\n",
    "#         torch.nn.init.xavier_uniform_(wts)\n",
    "#         biases = Variable(torch.zeros(1, int_vec_size), requires_grad=True)\n",
    "    \n",
    "#     return wts, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_model = _\n",
    "# heads = _\n",
    "# d_k = int(d_model/heads)\n",
    "\n",
    "#Same function is employed for encoder and decoder\n",
    "#inputs are padded - and padding mask ensures all padded words will have 0 vectors\n",
    "#All interaction with these pad vectors will result in values 0\n",
    "#once -inf mask is applied to them --> softmax assigns 0 probailities to these values\n",
    "#hence cutting off all possibilities/interactions\n",
    "#src_input --> comes from encoder side, trg_inp comes from decoder side\n",
    "#For encoder --> same/src_inp are passed twice\n",
    "#For decoder --> first step in each layer gets trg_inp for both, second step --> gets enc_output and dec_output\n",
    "\n",
    "#def mh_enc_dec(enc_out, dec_mmha_out, params, use_gpu, non_peek):\n",
    "def mh_enc_dec(src_inp, trg_inp, params, use_gpu, non_peek):\n",
    "    \n",
    "    #d_k = int(d_model/heads)\n",
    "    batch_size = src_inp.size(0)\n",
    "    Wq, Wk, Wv, Wo = params\n",
    "    \n",
    "    #represent all word-embeddings as intermediate vectors\n",
    "    q_vecs = trg_inp @ Wq     #Quaries come from decoder\n",
    "    k_vecs = src_inp @ Wk\n",
    "    v_vecs = src_inp @ Wv\n",
    "    \n",
    "    #reshape --> torch.Size([2, 2, 24, 15]) --> each sentence block has now h sets of int vectors --> or heads\n",
    "    q_vecs = q_vecs.view(batch_size, -1, heads, d_k).transpose(1, 2)\n",
    "    k_vecs = k_vecs.view(batch_size, -1, heads, d_k).transpose(1, 2)\n",
    "    v_vecs = v_vecs.view(batch_size, -1, heads, d_k).transpose(1, 2)\n",
    "    \n",
    "    #get raw scores, each block in q is dotted with transpose respective block in k\n",
    "    raw_wtss = torch.matmul(q_vecs, k_vecs.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    \n",
    "    #####################################\n",
    "    #Direct method of applying both pad and non_peak and not required to compute cortd_scores\n",
    "    #if non_peek:\n",
    "        #after_pad_n_non_peek = ((1 -(raw_wtss1==0)*1) * (non_peek_b(raw_wtss1))) * raw_wtss1 + (((((1 -(raw_wtss1==0)*1) * (non_peek_b(raw_wtss1))) * raw_wtss1)==0)*-1e9)\n",
    "    ###########################################\n",
    "    #get a mask with 0 or -inf where there are 0(padded) \n",
    "    #Add to raw_score then use softmax to get the attention weights\n",
    "    neg_inf_mask = (raw_wtss==0)*(-1e9)\n",
    "    \n",
    "    #ADD NON PEAK on top of this\n",
    "    cortd_scores = raw_wtss + neg_inf_mask\n",
    "    \n",
    "    if non_peek:\n",
    "        \n",
    "        cortd_scores = ((non_peek_mask(cortd_scores, use_gpu) * cortd_scores) + ((non_peek_mask(cortd_scores, use_gpu)*cortd_scores)==0)*-1e9)\n",
    "    soft_wtss =nn.Softmax(dim = -1)(cortd_scores)\n",
    "    \n",
    "    \n",
    "    \n",
    "    wtd_vals = soft_wtss @ v_vecs\n",
    "    wtd_vals = wtd_vals.transpose(1,2).contiguous().view(batch_size, -1, d_model) \n",
    "    \n",
    "    forward_vals = wtd_vals @ Wo\n",
    "    \n",
    "    return forward_vals\n",
    "    #return forward_vals, v_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#position wise feed forwad\n",
    "def pFFN(input_tensor, params):\n",
    "    eFF1a, eFF1b, eB1a, eB1b = params\n",
    "    first_lin = nn.ReLU()((input_tensor @ eFF1a) + eB1a)\n",
    "    out_tensor = (first_lin @ eFF1b) + eB1b\n",
    "    return out_tensor\n",
    "\n",
    "\n",
    "#Final linear before softmax\n",
    "def fin_linear(input_tensor, params):\n",
    "    wts, biases = params\n",
    "    out_tensor = (input_tensor @ wts) + biases\n",
    "    return out_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize each vector\n",
    "def layer_norm(x):\n",
    "    \n",
    "    row_mean = torch.mean(x, dim =-1)\n",
    "    row_var = torch.var(x, dim =-1)\n",
    "    row_std = torch.std(x, dim =-1)\n",
    "    row_norm_x = (x - row_mean.view(x.size()[0], x.size(1), 1))/row_std.view(x.size()[0], x.size(1), 1)\n",
    "    return row_norm_x\n",
    "\n",
    "#Residual connection\n",
    "def add_norm(original, transformed):\n",
    "    added = original + transformed\n",
    "    add_norm = layer_norm(added)\n",
    "    return add_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get non peek mask, compatible with score matrix\n",
    "def non_peek_mask(score_matrix, use_gpu):\n",
    "    low_t = torch.tril(torch.ones(score_matrix.size()), diagonal=0).int()\n",
    "    \n",
    "    if use_gpu:\n",
    "        return low_t.type(dtype).to(device)\n",
    "    else:\n",
    "        return low_t.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining releant steps together\n",
    "\n",
    "#For the whole corpus\n",
    "#Break sentences into list of words, add 'sos' at the beginning and 'eos' at the end\n",
    "#Get vocab, dictionaries of word - idx conversion, the length of longest sentence, raw embedding weigts\n",
    "def process_and_embed(raw_txt, lang, d_model):\n",
    "    words_in_sent_seos, vocab, idx_2_word, word_2_idx, most_common, maxList, maxLength = pre_process(raw_txt, lang)\n",
    "    emb_dim = d_model\n",
    "    embeds = get_embeds(vocab, d_model)\n",
    "    return (words_in_sent_seos, vocab, idx_2_word, word_2_idx, most_common, maxList, maxLength, embeds)\n",
    "\n",
    "\n",
    "\n",
    "#Get a slice of data, startig from ith index\n",
    "def get_batch(i, sent_list, batch_size):\n",
    "    cur_batch = sent_list[i:i+batch_size]\n",
    "    return cur_batch\n",
    "\n",
    "#shuffled_list = xxx, (randomize the total samples)\n",
    "def get_batch_shuffled(i, sent_list, batch_size):\n",
    "    idxs = shuffled_list[i*batch_size:i*batch_size+batch_size]\n",
    "    cur_batch = list(np.array(sent_list)[idxs])\n",
    "    return cur_batch\n",
    "\n",
    "\n",
    "#Change the words to corresponding embedding\n",
    "#Get the target (shifted) words as integers, to be used for loss computation\n",
    "def get_inp_tensor(cur_batch, embeds_look_up, word_2_idx, use_gpu):\n",
    "    sent_to_idx_bi, len_each_sent_bi, batch_tensor_bi = sent_2_idxs(cur_batch, embeds_look_up, word_2_idx)\n",
    "    cur_inp = add_pos_enc(batch_tensor_bi)\n",
    "    \n",
    "    #Restore the 0 values despite embeddings/pos encodings being non 0s\n",
    "    mask_using_len = get_mask_by_length(len_each_sent_bi)  ### alternative\n",
    "    masked_inp = cur_inp * mask_using_len[:, :, None]\n",
    "#     mask_using_batch = get_mask_by_batch(batch_tensor_bi)\n",
    "\n",
    "    \n",
    "    #targets start after sos and target for eos in longest sentence is a pad(0)\n",
    "    targets = [(i[1:] + [0]) for i in sent_to_idx_bi]\n",
    "    if use_gpu:\n",
    "        return masked_inp.type(dtype).to(device), targets#.to(device)\n",
    "        \n",
    "    else:\n",
    "        return masked_inp, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute multi-head attention using encoder\n",
    "#Note multi-head for encoder has one step and does not use Non-peek mask\n",
    "def compute_mha_enc(src_inp, kqv_weights, ffn_params, use_gpu):\n",
    "    \n",
    "    enc_mh_out = mh_enc_dec(src_inp, src_inp, kqv_weights, use_gpu, non_peek =False ) \n",
    "    add_norm_enc_mh_out = add_norm(src_inp, enc_mh_out)\n",
    "    ffn_out1 = pFFN(add_norm_enc_mh_out, ffn_params)\n",
    "    final_dec_out1 = add_norm(add_norm_enc_mh_out, ffn_out1)\n",
    "    return final_dec_out1\n",
    "\n",
    "\n",
    "\n",
    "#compute multi-head attention using encoder\n",
    "#Note multi-head for decoder has two step and uses Non-peek mask for first step\n",
    "def compute_mha_dec(trg_inp, enc_out, kqv_weights1, kqv_weights2, ffn_params, use_gpu):\n",
    "    \n",
    "    dec_masked_mh_out = mh_enc_dec(trg_inp, trg_inp, kqv_weights1, use_gpu, non_peek=True) \n",
    "    add_norm_dec_masked_mh_out = add_norm(trg_inp, dec_masked_mh_out)\n",
    "    \n",
    "    #Decoder stack is used for residual connection\n",
    "    dec_multi_h_out = mh_enc_dec(enc_out, add_norm_dec_masked_mh_out, kqv_weights2, use_gpu, non_peek=False) \n",
    "    add_norm_dec_multi_h_out = add_norm(add_norm_dec_masked_mh_out, dec_multi_h_out)\n",
    "    \n",
    "    ffn_out = pFFN(add_norm_dec_multi_h_out, ffn_params)\n",
    "    final_dec_out = add_norm(add_norm_dec_multi_h_out, ffn_out)\n",
    "    return final_dec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the key variables\n",
    "#By tweaking these number --> words can be repesented as any dim, different heads and int vectors (used in pFFN)\n",
    "d_model = 48  #\n",
    "heads = 2\n",
    "d_k = int(d_model/heads)\n",
    "int_vec_size = 96\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d_model = 64\n",
    "\n",
    "#Read the sentences in seperate text files\n",
    "with open ('short_nep//nep_337_21_nov.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    short_nepali_txt = f.read()\n",
    "    \n",
    "with open ('short_nep//short_eng_nep_nov_21.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    short_eng_txt = f.read() \n",
    "\n",
    "#For both english and nepali --> get sentences, dictionaries, vocabs and embeddings\n",
    "#lang = 'eng'\n",
    "words_in_sent_seos_e, vocab_e, idx_2_word_e, word_2_idx_e, most_common_e, maxList_e, maxLength_e, embeds_e = \\\n",
    "process_and_embed(short_eng_txt, 'eng', d_model)\n",
    "\n",
    "#Just as important\n",
    "word_2_idx_e.update( {'pad' : 0} )\n",
    "idx_2_word_e[0] ='pad'\n",
    "\n",
    "\n",
    "\n",
    "#lang = 'nep'\n",
    "words_in_sent_seos_n, vocab_n, idx_2_word_n, word_2_idx_n, most_common_n, maxList_n, maxLength_n, embeds_n = \\\n",
    "process_and_embed(short_nepali_txt, 'nep', d_model)\n",
    "\n",
    "#Just as important\n",
    "word_2_idx_n.update( {'pad' : 0} )\n",
    "idx_2_word_n[0] ='pad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[286, 292, 56, 233, 329, 261, 253, 73, 109, 302]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#shuffle_list cotains all the numbers from sentence 0 to end\n",
    "losses = []\n",
    "shuffled_list = [i for i in range(len(words_in_sent_seos_e))]\n",
    "random.shuffle(shuffled_list)\n",
    "shuffled_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant states\n",
    "# d_model = _\n",
    "# int_vec_size = _\n",
    "# use_gpu = True/False\n",
    "\n",
    "#Encoder parameters\n",
    "params_enc_mh1 = list(init_weights(d_model, use_gpu))\n",
    "params_ffn_enc_mh1 = list(init_FFN(d_model, int_vec_size, use_gpu))\n",
    "params_enc_mh2 = list(init_weights(d_model, use_gpu))\n",
    "params_ffn_enc_mh2 = list(init_FFN(d_model, int_vec_size, use_gpu))\n",
    "\n",
    "\n",
    "#Decoder parameters\n",
    "params_dec_mh1a = list(init_weights(d_model, use_gpu))\n",
    "params_dec_mh1b = list(init_weights(d_model, use_gpu))\n",
    "params_ffn_dec_mh1 = list(init_FFN(d_model, int_vec_size, use_gpu))\n",
    "params_dec_mh2a = list(init_weights(d_model, use_gpu))\n",
    "params_dec_mh2b = list(init_weights(d_model, use_gpu))\n",
    "params_ffn_dec_mh2 = list(init_FFN(d_model, int_vec_size, use_gpu))\n",
    "\n",
    "\n",
    "#fin layer of decoder\n",
    "fin_params = list(init_fin_FFN(d_model, vocab_n, use_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a list of all parameters\n",
    "#use Adam as opitmizer\n",
    "#Note initial embedding are neither pre-trained nor tied in this simple model\n",
    "all_parameters = params_enc_mh1 + params_ffn_enc_mh1 + params_enc_mh2 + params_ffn_enc_mh2 + params_dec_mh1a + params_dec_mh1b + params_ffn_dec_mh1 + \\\n",
    "                   params_dec_mh2a + params_dec_mh2b + params_ffn_dec_mh2 + fin_params # + list(embeds_n.weight.data) + list(embeds_e.weight.data)\n",
    "\n",
    "optimizer = torch.optim.Adam((all_parameters), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "6.1360249519348145\n",
      "5\n",
      "5.505064010620117\n",
      "10\n",
      "5.175290584564209\n",
      "15\n",
      "4.913086891174316\n",
      "20\n",
      "4.655522346496582\n",
      "25\n",
      "4.393017768859863\n",
      "30\n",
      "4.13569450378418\n",
      "35\n",
      "3.8869175910949707\n",
      "40\n",
      "3.6489763259887695\n",
      "45\n",
      "3.418710947036743\n",
      "50\n",
      "3.1960952281951904\n",
      "55\n",
      "2.9813826084136963\n",
      "60\n",
      "2.7736191749572754\n",
      "65\n",
      "2.5756564140319824\n",
      "70\n",
      "2.389052629470825\n",
      "75\n",
      "2.2125189304351807\n",
      "80\n",
      "2.0480284690856934\n",
      "85\n",
      "1.895870566368103\n",
      "90\n",
      "1.753072738647461\n",
      "95\n",
      "1.6203293800354004\n",
      "61.562521200001356\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "start = timer()\n",
    "#ALl inputs are and weights (appropriately typed) are shifted to GPU - when GPU is preferred\n",
    "# heads = _\n",
    "# d_model = _\n",
    "# d_k = int(d_model/heads)\n",
    "# int_vec_size = _\n",
    "batch_size = 48   \n",
    "#random.shuffle(shuffled_list)\n",
    "\n",
    "use_gpu = True\n",
    "for j in range(100):\n",
    "    for i in range(len(words_in_sent_seos_e)//batch_size):\n",
    "    \n",
    "        cur_batch_eng = get_batch(i, words_in_sent_seos_e, batch_size)\n",
    "        cur_batch_nep = get_batch(i, words_in_sent_seos_n, batch_size)\n",
    "        \n",
    "        #Alternatively shuffle the input sentences\n",
    "#         cur_batch_eng = get_batch_shuffled(i, words_in_sent_seos_e, batch_size)\n",
    "#         cur_batch_nep = get_batch_shuffled(i, words_in_sent_seos_n, batch_size)\n",
    "    \n",
    "        \n",
    "        masked_inp_eng, word_indices_eng = get_inp_tensor(cur_batch_eng, embeds_e, word_2_idx_e, use_gpu)\n",
    "\n",
    "        #For encoder side --> non_peek shouldn't be applied\n",
    "        final_enc_out1 = compute_mha_enc(masked_inp_eng, params_enc_mh1, params_ffn_enc_mh1, use_gpu)\n",
    "        final_enc_out2 = compute_mha_enc(final_enc_out1, params_enc_mh2, params_ffn_enc_mh2, use_gpu)\n",
    "\n",
    "\n",
    "        #TARGET SENTENCES\n",
    "        #word_indices are the targets in context of lang that is being decoded\n",
    "        #All the indices are right shifted, along with non-peek, and teacher forcing --> trains the decoder\n",
    "        masked_inp_nep, word_indices_nep = get_inp_tensor(cur_batch_nep, embeds_n, word_2_idx_n, use_gpu)\n",
    "\n",
    "\n",
    "        #Decoder computation\n",
    "        final_dec_out1 = compute_mha_dec(masked_inp_nep, final_enc_out2, params_dec_mh1a, params_dec_mh1b, params_ffn_dec_mh1, use_gpu)\n",
    "        final_dec_out2 = compute_mha_dec(final_dec_out1, final_enc_out2, params_dec_mh2a, params_dec_mh2b, params_ffn_dec_mh2, use_gpu)\n",
    "        \n",
    "        #From the top of the decoder        \n",
    "        fin_tensor = fin_linear(final_dec_out2, fin_params)\n",
    "        \n",
    "        #Shifterd to GPU\n",
    "        targets_nep = torch.tensor(word_indices_nep).view(-1).to(device)\n",
    "        \n",
    "        #loss = F.cross_entropy(fin_tensor.view(-1, fin_tensor.size(-1)), targets_nep)\n",
    "        #Computes loss excluding pads\n",
    "        loss_ex_pad =  F.cross_entropy(fin_tensor.view(-1, fin_tensor.size(-1)), targets_nep, ignore_index= 0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_ex_pad.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        if i%4 ==0:\n",
    "            losses.append(loss_ex_pad.item())\n",
    "            \n",
    "    \n",
    "    if j%5 ==0:\n",
    "        print(j)\n",
    "        print(loss_ex_pad.item())\n",
    "        \n",
    "\n",
    "end = timer()\n",
    "elapsed_time = end - start\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17323f1ddc8>]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once trained, ready to translate\n",
    "#Translation - Encoder computes the stack for whole source sentence\n",
    "#Decoder - decodes the first word --> given encoder stack and 'nep_SOS' --> second word --> given encoder stack + 'sos' + first word....so forth\n",
    "#The embedding for 'nep_sos' is used to kick start decoding\n",
    "#Translation function uses the same logic except batch size is kept 1 (it's possible to translate multiple sentences at once)\n",
    "batch_size = 1\n",
    "def trans_2_nep(sent_num):\n",
    "   \n",
    "    eng_sent = get_batch(sent_num, words_in_sent_seos_e, 1)\n",
    "    starter = embeds_n(torch.LongTensor([word_2_idx_n['एसओएस']])).view(1, 1, -1).type(dtype).to(device)\n",
    "    masked_inp_src, word_indices_src = get_inp_tensor(eng_sent, embeds_e, word_2_idx_e, use_gpu)\n",
    "    \n",
    "    print(get_batch(sent_num, words_in_sent_seos_e, 1))\n",
    "    for i in range(10):\n",
    "\n",
    "\n",
    "        \n",
    "        final_enc_out_trans1 = compute_mha_enc(masked_inp_src, params_enc_mh1, params_ffn_enc_mh1, use_gpu)\n",
    "        final_enc_out_trans2 = compute_mha_enc(final_enc_out_trans1, params_enc_mh2, params_ffn_enc_mh2, use_gpu)\n",
    "        \n",
    "\n",
    "        \n",
    "        final_dec_out1 = compute_mha_dec(starter, final_enc_out_trans2, params_dec_mh1a, params_dec_mh1b, params_ffn_dec_mh1, use_gpu)\n",
    "        final_dec_out2 = compute_mha_dec(final_dec_out1, final_enc_out_trans2, params_dec_mh2a, params_dec_mh2b, params_ffn_dec_mh2, use_gpu)\n",
    "        \n",
    "\n",
    "        fin_tensor = fin_linear(final_dec_out2, fin_params)\n",
    "        y_hat = nn.Softmax(dim =-1)(fin_tensor)\n",
    "        pred_next = torch.topk(y_hat.view(-1, y_hat.size(-1)), 3)[1][i,0]\n",
    "        emb_pred_word = embeds_n(torch.LongTensor([pred_next.item()])).view(1, 1, -1).type(dtype).to(device)\n",
    "        starter = torch.cat((starter, emb_pred_word), dim =1)\n",
    "        nepali_word = idx_2_word_n[pred_next.item()]\n",
    "        print(nepali_word)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if nepali_word == 'ईओएस':\n",
    "            break\n",
    "    return (torch.topk(y_hat.view(-1, y_hat.size(-1)), 4))  #print top indices and their probailities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['start_o_s', 'get', 'ready', 'to', 'eat', 'end_o_s']]\n",
      "खान\n",
      "तयार\n",
      "हुनुहोस्\n",
      "ईओएस\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[0.0595, 0.0242, 0.0202, 0.0198],\n",
       "        [0.0470, 0.0294, 0.0174, 0.0135],\n",
       "        [0.1360, 0.0173, 0.0131, 0.0128],\n",
       "        [0.4343, 0.0351, 0.0144, 0.0124]], device='cuda:0',\n",
       "       grad_fn=<TopkBackward>),\n",
       "indices=tensor([[274, 192, 239, 486],\n",
       "        [486, 274, 487, 192],\n",
       "        [192, 273, 487, 456],\n",
       "        [487, 112, 456, 273]], device='cuda:0'))"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_2_nep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 8, 32])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_inp_eng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mh_enc_dec11(src_inp, trg_inp, params, use_gpu, non_peek):\n",
    "    \n",
    "    #d_k = int(d_model/heads)\n",
    "    batch_size = src_inp.size(0)\n",
    "    Wq, Wk, Wv, Wo = params\n",
    "    \n",
    "    #represent all word-embeddings as intermediate vectors\n",
    "    q_vecs = trg_inp @ Wq     #Quaries come from decoder\n",
    "    k_vecs = src_inp @ Wk\n",
    "    v_vecs = src_inp @ Wv\n",
    "    \n",
    "    #reshape --> torch.Size([2, 2, 24, 15]) --> each sentence block has now h sets of int vectors --> or heads\n",
    "    q_vecs = q_vecs.view(batch_size, -1, heads, d_k).transpose(1, 2)\n",
    "    k_vecs = k_vecs.view(batch_size, -1, heads, d_k).transpose(1, 2)\n",
    "    v_vecs = v_vecs.view(batch_size, -1, heads, d_k).transpose(1, 2)\n",
    "    \n",
    "    #get raw scores, each block in q is dotted with transpose respective block in k\n",
    "    raw_wtss = torch.matmul(q_vecs, k_vecs.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    \n",
    "    #####################################\n",
    "    #Direct method of applying both pad and non_peak and not required to compute cortd_scores\n",
    "    #if non_peek:\n",
    "        #after_pad_n_non_peek = ((1 -(raw_wtss1==0)*1) * (non_peek_b(raw_wtss1))) * raw_wtss1 + (((((1 -(raw_wtss1==0)*1) * (non_peek_b(raw_wtss1))) * raw_wtss1)==0)*-1e9)\n",
    "    ###########################################\n",
    "    #get a mask with 0 or -inf where there are 0(padded) \n",
    "    #Add to raw_score then use softmax to get the attention weights\n",
    "    neg_inf_mask = (raw_wtss==0)*(-1e9)\n",
    "    \n",
    "    #ADD NON PEAK on top of this\n",
    "    cortd_scores = raw_wtss + neg_inf_mask\n",
    "    \n",
    "    if non_peek:\n",
    "        \n",
    "        cortd_scores = ((non_peek_mask(cortd_scores, use_gpu) * cortd_scores) + ((non_peek_mask(cortd_scores, use_gpu)*cortd_scores)==0)*-1e9)\n",
    "    soft_wtss =nn.Softmax(dim = -1)(cortd_scores)\n",
    "    \n",
    "    \n",
    "    \n",
    "    wtd_vals = soft_wtss @ v_vecs\n",
    "    wtd_vals = wtd_vals.transpose(1,2).contiguous().view(batch_size, -1, d_model) \n",
    "    \n",
    "    forward_vals = wtd_vals @ Wo\n",
    "    \n",
    "    #return forward_vals\n",
    "    return forward_vals, v_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 4, 8, 16])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, kkk = mh_enc_dec11(masked_inp_eng, masked_inp_eng, params_enc_mh1, use_gpu, False)\n",
    "kkk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 8, 32])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_inp_eng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "48*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_enc_mh1[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
